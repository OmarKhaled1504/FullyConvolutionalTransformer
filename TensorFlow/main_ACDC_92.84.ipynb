{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from imps import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes when running on the cluster\n",
    "acdc_data = \"/nfs/\"+acdc_data[8:] \n",
    "\n",
    "acdc_data_train = acdc_data + '/train'\n",
    "acdc_data_test = acdc_data + '/test'\n",
    "acdc_data_validation = acdc_data + '/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep_gen(data, ismask, seed=seed, batch_size=15, dset=\"training\"):\n",
    "    if dset==\"training\":\n",
    "        if ismask:\n",
    "            datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                rotation_range=20,\n",
    "                width_shift_range=.2,\n",
    "                height_shift_range=.2,\n",
    "                horizontal_flip=True,\n",
    "                vertical_flip=True, \n",
    "                preprocessing_function = lambda x: np.where(x>0, 1, 0).astype(x.dtype),\n",
    "                )\n",
    "        else:\n",
    "            datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                rotation_range=20,\n",
    "                width_shift_range=.2,\n",
    "                height_shift_range=.2,\n",
    "                horizontal_flip=True,\n",
    "                vertical_flip=True,     \n",
    "                )\n",
    "    elif dset==\"validation\":\n",
    "        if ismask:\n",
    "            datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "               preprocessing_function = lambda x: np.where(x>0, 1, 0).astype(x.dtype),\n",
    "                )\n",
    "            \n",
    "        else:\n",
    "            datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "    else:\n",
    "        raise ValueError( \"The argument \\\"dset\\\" can either be \\\"training\\\" or \\\"validation\\\".\")\n",
    "    \n",
    "    return datagen.flow(data, batch_size=batch_size, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Get data ACDC\n",
    "\n",
    "\n",
    "# training\n",
    "acdc_data, train_afn, train_hdr = get_acdc(acdc_data_train)\n",
    "X_train, y_train, info_train = acdc_data[0], acdc_data[1], acdc_data[2]\n",
    "# validation\n",
    "acdc_data, val_afn, val_hdr = get_acdc(acdc_data_validation)\n",
    "X_val, y_val, info_val = acdc_data[0], acdc_data[1], acdc_data[2]\n",
    "# testing\n",
    "acdc_data, test_afn, test_hdr = get_acdc(acdc_data_test)\n",
    "X_test, y_test, info_test = acdc_data[0], acdc_data[1], acdc_data[2]\n",
    "\n",
    "y_train_cat = convert_masks(y_train)\n",
    "y_val_cat = convert_masks(y_val)\n",
    "y_test_cat = convert_masks(y_test)\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_generator=unite_gen(X_train, y_train_cat[:,::4,::4,:], y_train_cat[:,::2,::2,:], y_train_cat, batch_size, \"training\")\n",
    "val_generator=unite_gen(X_val, y_val_cat[:,::4,::4,:], y_val_cat[:,::2,::2,:], y_val_cat, batch_size, \"validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myunet(data):\n",
    "    \n",
    "    training = True # flag\n",
    "\n",
    "    att_heads = [2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
    "    filters = [32, 64, 128, 256, 512, 256, 128, 64, 32]\n",
    "    #\n",
    "    blocks = len(filters)\n",
    "\n",
    "    stochastic_depth_rate = 0.0\n",
    "    \n",
    "    image_size = data.shape[1]\n",
    "    \n",
    "    input_shape = (data.shape[1], data.shape[2], data.shape[3])\n",
    "    \n",
    "\n",
    "    \n",
    "    class StochasticDepth(layers.Layer):\n",
    "        def __init__(self, drop_prop, **kwargs):\n",
    "            super(StochasticDepth, self).__init__(**kwargs)\n",
    "            self.drop_prob = drop_prop\n",
    "    \n",
    "        def call(self, x, training=training):\n",
    "            if training:\n",
    "                keep_prob = 1 - self.drop_prob\n",
    "                shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
    "                random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
    "                random_tensor = tf.floor(random_tensor)\n",
    "                return (x / keep_prob) * random_tensor\n",
    "            return x\n",
    "    \n",
    "    def mlp(x, hidden_units, dropout_rate): \n",
    "        x1 = layers.Conv2D(units, 3, padding='same', activation=tf.nn.gelu)(x)\n",
    "        x1 = layers.Dropout(0.1)(x1)\n",
    "        x2 = layers.Conv2D(units, 3, padding='same', dilation_rate=2, activation=tf.nn.gelu)(x)\n",
    "        x2 = layers.Dropout(0.1)(x2)\n",
    "        x3 = layers.Conv2D(units, 3, padding='same', dilation_rate=3, activation=tf.nn.gelu)(x)\n",
    "        x3 = layers.Dropout(0.1)(x3)\n",
    "        added = layers.Add()([x1,x2, x3])\n",
    "        x_out = layers.Conv2D(units, 3, padding='same', activation=tf.nn.gelu)(added)\n",
    "        x_out = layers.Dropout(0.1)(x_out)\n",
    "        return x_out\n",
    "\n",
    "    from tensorflow.keras.layers import Layer, Dense, Conv2D, Dropout, MultiHeadAttention, BatchNormalization, \\\n",
    "    DepthwiseConv2D, UpSampling2D\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow import Tensor, divide, concat, random, split, reshape, transpose, float32\n",
    "    from typing import List, Union, Iterable\n",
    "    \n",
    "        \n",
    "    class Attention(Layer):\n",
    "        def __init__(self,\n",
    "                      # dim_in,\n",
    "                      dim_out,\n",
    "                      num_heads,\n",
    "                      proj_drop=0.0,#2,\n",
    "                      kernel_size=3,\n",
    "                      stride_kv=1, \n",
    "                      stride_q=1, \n",
    "                      padding_kv=\"same\",\n",
    "                      padding_q=\"same\",\n",
    "                      attention_bias=True):\n",
    "            super().__init__()\n",
    "            self.stride_kv = stride_kv\n",
    "            self.stride_q = stride_q\n",
    "            self.dim = dim_out\n",
    "            self.num_heads = num_heads\n",
    "            self.scale = dim_out ** -0.5\n",
    "    \n",
    "            self.conv_proj_q = self._build_projection(kernel_size, stride_q, padding_q)\n",
    "            self.conv_proj_k = self._build_projection(kernel_size, stride_kv, padding_kv)\n",
    "            self.conv_proj_v = self._build_projection(kernel_size, stride_kv, padding_kv)\n",
    "    \n",
    "            self.attention = MultiHeadAttention(self.num_heads, dim_out, use_bias=attention_bias)\n",
    "            self.proj_drop = Dropout(proj_drop)\n",
    "            \n",
    "    \n",
    "        @staticmethod\n",
    "        def _build_projection(kernel_size, stride, padding):\n",
    "            proj = Sequential([DepthwiseConv2D(kernel_size, padding=padding, strides=stride, use_bias=False),\n",
    "                layers.LayerNormalization(),                \n",
    "            ])\n",
    "            return proj\n",
    "     \n",
    "    \n",
    "        def call_conv(self, x, h, w):\n",
    "            q = self.conv_proj_q(x)\n",
    "            k = self.conv_proj_k(x)\n",
    "            v = self.conv_proj_v(x)\n",
    "            return q, k, v\n",
    "    \n",
    "        def call(self, inputs, mask=None, training=training, h=1, w=1):\n",
    "            x = inputs\n",
    "            q, k, v = self.call_conv(x, h, w)\n",
    "            x = self.attention(q, v, key=k)\n",
    "            if training:\n",
    "                x = self.proj_drop(x)            \n",
    "            return x\n",
    " \n",
    "    \n",
    "    def att(x_in, \n",
    "            num_heads,\n",
    "            dpr, \n",
    "            proj_drop=0.0,\n",
    "            attention_bias=True,\n",
    "            padding_q=\"same\",\n",
    "            padding_kv=\"same\",\n",
    "            stride_kv=2,\n",
    "            stride_q=1):\n",
    "    \n",
    "        b, h, w, c = x_in.shape\n",
    "    \n",
    "        attention_output = Attention(#dim_in=c,\n",
    "                                      dim_out=c,\n",
    "                                      num_heads=num_heads,\n",
    "                                      proj_drop=proj_drop,\n",
    "                                      attention_bias=attention_bias, \n",
    "                                      padding_q=padding_q, \n",
    "                                      padding_kv=padding_kv, \n",
    "                                      stride_kv=stride_kv, \n",
    "                                      stride_q=stride_q, \n",
    "                                      )(x_in, h=h, w=w, training=training, mask=None)\n",
    "        \n",
    "        \n",
    "        # Skip connection 1.\n",
    "        attention_output = StochasticDepth(dpr)(attention_output)\n",
    "        attention_output = Conv2D(x_in.shape[-1], 3, 1, padding=\"same\", activation=\"relu\")(attention_output)\n",
    "        x2 = layers.Add()([attention_output, x_in]) \n",
    "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
    "        x3 = mlp(x3, hidden_units=[c,c], dropout_rate=0.0)\n",
    "        # Skip connection 2.\n",
    "        x3 = StochasticDepth(dpr)(x3)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "        \n",
    "        return encoded_patches\n",
    "    \n",
    "    def create_model(\n",
    "        image_size=image_size,\n",
    "        input_shape=input_shape,\n",
    "    ):\n",
    "    \n",
    "        inputs = layers.Input(input_shape)\n",
    "    \n",
    "        dpr = [x for x in np.linspace(0, stochastic_depth_rate, blocks)]\n",
    "    \n",
    "        initializer = 'he_normal'\n",
    "        drp_out = 0.3\n",
    "        act = \"relu\"    \n",
    "    \n",
    "        scale_img_2 = layers.AveragePooling2D(2,2)(inputs)\n",
    "        scale_img_3 = layers.AveragePooling2D(2,2)(scale_img_2)\n",
    "        scale_img_4 = layers.AveragePooling2D(2,2)(scale_img_3)\n",
    "       \n",
    "       \n",
    "        # first block\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(inputs[:,:,:,-1])\n",
    "        x11 = tf.expand_dims(x1, -1)                                \n",
    "        x11 = Conv2D(filters[0], 3, 1, padding=\"same\", activation=act, kernel_initializer=initializer)(x11)\n",
    "        x11 = Conv2D(filters[0], 3, 1, padding=\"same\", activation=act, kernel_initializer=initializer)(x11)\n",
    "        x11 = Dropout(drp_out)(x11)\n",
    "        x11 = MaxPooling2D((2,2))(x11)\n",
    "        out = att(x11, att_heads[0], dpr[0])\n",
    "        skip1=out\n",
    "        #print(\"\\nBlock 1 -> input:\", x1.shape, \"output:\", skip1.shape)\n",
    "        \n",
    "        # second block\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(out)\n",
    "        x11=x1\n",
    "        x11 = concatenate([Conv2D(filters[0], 3, padding=\"same\", activation=act)(scale_img_2), x11], axis=3)\n",
    "        x11 = Conv2D(filters[1], 3, 1, padding=\"same\", activation=act, kernel_initializer=initializer)(x11)\n",
    "        x11 = Conv2D(filters[1], 3, 1, padding=\"same\", activation=act, kernel_initializer=initializer)(x11)\n",
    "        x11 = Dropout(drp_out)(x11)\n",
    "        x11 = MaxPooling2D((2,2))(x11)\n",
    "        out = att(x11, att_heads[1], dpr[1])\n",
    "        skip2=out\n",
    "        #print(\"Block 2 -> input:\", x1.shape, \"output:\", skip2.shape)\n",
    "        \n",
    "        # third block\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(out)\n",
    "        x11=x1\n",
    "        x11 = concatenate([Conv2D(filters[1], 3, padding=\"same\", activation=act)(scale_img_3), x11], axis=3)\n",
    "        x11 = Conv2D(filters[2], 3, 1, padding=\"same\", activation=act, kernel_initializer=initializer)(x11)\n",
    "        x11 = Conv2D(filters[2], 3, 1, padding=\"same\", activation=act, kernel_initializer=initializer)(x11)\n",
    "        x11 = Dropout(drp_out)(x11)\n",
    "        x11 = MaxPooling2D((2,2))(x11)\n",
    "        out = att(x11, att_heads[2], dpr[2])\n",
    "        skip3=out\n",
    "        #print(\"Block 3 -> input:\", x1.shape, \"output:\", skip3.shape)\n",
    "        \n",
    "        # fourth block\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(out)\n",
    "        x11=x1\n",
    "        x11 = concatenate([Conv2D(filters[2], 3, padding=\"same\", activation=act)(scale_img_4), x11], axis=3)\n",
    "        x11 = Conv2D(filters[3], 3, 1, padding=\"same\", activation=act, kernel_initializer=initializer)(x11)\n",
    "        x11 = Conv2D(filters[3], 3, 1, padding=\"same\", activation=act, kernel_initializer=initializer)(x11)\n",
    "        x11 = Dropout(drp_out)(x11)\n",
    "        x11 = MaxPooling2D((2,2))(x11)\n",
    "        out = att(x11, att_heads[3], dpr[3])\n",
    "        skip4 = out\n",
    "        #print(\"Block 4 -> input:\", x1.shape, \"output:\", skip4.shape)\n",
    "         \n",
    "        # fifth block\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(out)\n",
    "        x11=x1\n",
    "        x11 = Conv2D(filters[4], 3, 1, padding=\"same\", activation=act, kernel_initializer=initializer)(x11)\n",
    "        x11 = Conv2D(filters[4], 3, 1, padding=\"same\", activation=act, kernel_initializer=initializer)(x11)\n",
    "        x11 = Dropout(drp_out)(x11)\n",
    "        x11 = MaxPooling2D((2,2))(x11)\n",
    "        out = att(x11, att_heads[4], dpr[4])\n",
    "        # print(\"Block 5 -> input:\", x1.shape, \"output:\", tf.reshape(out, (tf.shape(out)[0], np.sqrt(out.get_shape()[1]), np.sqrt(out.get_shape()[1]), out.get_shape()[2])).shape) \n",
    "        \n",
    "        # sixth block\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(out)\n",
    "        x11=x1\n",
    "        x11 = Conv2D(filters[5], 2, padding=\"same\", activation=act, kernel_initializer=initializer)(UpSampling2D(size=(2,2))(x11))\n",
    "        x11 = concatenate([skip4,x11], axis=3)\n",
    "        x11 = Conv2D(filters[5], 3, 1, padding=\"same\", activation=act, kernel_initializer=initializer)(x11)\n",
    "        x11 = Conv2D(filters[5], 3, 1, padding=\"same\", activation=act, kernel_initializer=initializer)(x11)\n",
    "        x11 = Dropout(drp_out)(x11)\n",
    "        out = att(x11, att_heads[5], dpr[5])\n",
    "        # print(\"Block 6 -> input:\", x1.shape, \"output:\", tf.reshape(out, (tf.shape(out)[0], np.sqrt(out.get_shape()[1]), np.sqrt(out.get_shape()[1]), out.get_shape()[2])).shape)                 \n",
    "        \n",
    "        # seventh block\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(out)\n",
    "        x11=x1\n",
    "        x11 = Conv2D(filters[6], 2, padding=\"same\", activation=act, kernel_initializer=initializer)(UpSampling2D(size=(2,2))(x11))\n",
    "        x11 = concatenate([skip3,x11], axis=3)\n",
    "        x11 = Conv2D(filters[6], 3, 1, padding=\"same\", activation=act, kernel_initializer=initializer)(x11)\n",
    "        x11 = Conv2D(filters[6], 3, 1, padding=\"same\", activation=act, kernel_initializer=initializer)(x11)\n",
    "        x11 = Dropout(drp_out)(x11)\n",
    "        out = att(x11, att_heads[6], dpr[6])\n",
    "        skip7=out\n",
    "        # print(\"Block 7 -> input:\", x1.shape, \"output:\", skip7.shape)  \n",
    "        \n",
    "        # eighth block\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(out)\n",
    "        x11=x1\n",
    "        x11 = Conv2D(filters[7], 2, padding=\"same\", activation=act, kernel_initializer=initializer)(UpSampling2D(size=(2,2))(x11))\n",
    "        x11 = concatenate([skip2, x11], axis=3)\n",
    "        x11 = Conv2D(filters[7], 3, 1, padding=\"same\", activation=act, kernel_initializer=initializer)(x11)\n",
    "        x11 = Conv2D(filters[7], 3, 1, padding=\"same\", activation=act, kernel_initializer=initializer)(x11)\n",
    "        x11 = Dropout(drp_out)(x11)\n",
    "        out = att(x11, att_heads[7], dpr[7])\n",
    "        skip8=out\n",
    "        #print(\"Block 8 -> input:\", x1.shape, \"output:\", skip8.shape) \n",
    "        \n",
    "        # nineth block\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(out)\n",
    "        x11=x1\n",
    "        x11 = Conv2D(filters[8], 2, padding=\"same\", activation=act, kernel_initializer=initializer)(UpSampling2D(size=(2,2))(x11))\n",
    "        x11 = concatenate([skip1, x11], axis=3)\n",
    "        x11 = Conv2D(filters[8], 3, 1, padding=\"same\", activation=act, kernel_initializer=initializer)(x11)\n",
    "        x11 = Conv2D(filters[8], 3, 1, padding=\"same\", activation=act, kernel_initializer=initializer)(x11)\n",
    "        x11 = Dropout(drp_out)(x11)\n",
    "        out = att(x11, att_heads[8], dpr[8])\n",
    "        skip9=out\n",
    "        print(\"Block 9 -> input:\", x1.shape, \"output:\", skip9.shape) \n",
    "\n",
    "\n",
    "        skip7 = layers.LayerNormalization(epsilon=1e-5)(UpSampling2D(size=(2,2))(skip7))\n",
    "        out7 = Conv2D(filters[6], 3, padding=\"same\", activation=act, kernel_initializer=initializer)(skip7)\n",
    "        out7 = Conv2D(filters[6], 3, padding=\"same\", activation=act, kernel_initializer=initializer)(out7)\n",
    "        #\n",
    "        skip8 = layers.LayerNormalization(epsilon=1e-5)(UpSampling2D(size=(2,2))(skip8))\n",
    "        out8 = Conv2D(filters[7], 3, padding=\"same\", activation=act, kernel_initializer=initializer)(skip8)\n",
    "        out8 = Conv2D(filters[7], 3, padding=\"same\", activation=act, kernel_initializer=initializer)(out8)\n",
    "        #\n",
    "        skip9 = layers.LayerNormalization(epsilon=1e-5)(UpSampling2D(size=(2,2))(skip9))\n",
    "        out9 = Conv2D(filters[8], 3, padding=\"same\", activation=act, kernel_initializer=initializer)(skip9)\n",
    "        out9 = Conv2D(filters[8], 3, padding=\"same\", activation=act, kernel_initializer=initializer)(out9)\n",
    "        #\n",
    "\n",
    "        # # ACDC\n",
    "        out7 = Conv2D(4, (1,1), activation=\"sigmoid\", name='pred1')(out7)\n",
    "        out8 = Conv2D(4, (1,1), activation=\"sigmoid\", name='pred2')(out8)\n",
    "        out9 = Conv2D(4, (1,1), activation=\"sigmoid\", name='final')(out9)\n",
    "\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"DS 1 -> input:\", skip7.shape, \"output:\", out7.shape) \n",
    "        print(\"DS 2 -> input:\", skip8.shape, \"output:\", out8.shape) \n",
    "        print(\"DS 3 -> input:\", skip9.shape, \"output:\", out9.shape) \n",
    "        \n",
    "\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=[out7, out8, out9])\n",
    "\n",
    "        \n",
    "        return model    \n",
    "    return create_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf \n",
    "\n",
    "epsilon = 1e-5\n",
    "smooth = 1\n",
    "\n",
    "def dsc(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dsc(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "    loss = (2/10)*binary_crossentropy(y_true, y_pred) + (1/3)*dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def confusion(y_true, y_pred):\n",
    "    smooth=1\n",
    "    y_pred_pos = K.clip(y_pred, 0, 1)\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "    y_pos = K.clip(y_true, 0, 1)\n",
    "    y_neg = 1 - y_pos\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg) \n",
    "    prec = (tp + smooth)/(tp+fp+smooth)\n",
    "    recall = (tp+smooth)/(tp+fn+smooth)\n",
    "    return prec, recall\n",
    "\n",
    "def tp(y_true, y_pred):\n",
    "    smooth = 1\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    tp = (K.sum(y_pos * y_pred_pos) + smooth)/ (K.sum(y_pos) + smooth) \n",
    "    return tp \n",
    "\n",
    "def tn(y_true, y_pred):\n",
    "    smooth = 1\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos \n",
    "    tn = (K.sum(y_neg * y_pred_neg) + smooth) / (K.sum(y_neg) + smooth )\n",
    "    return tn \n",
    "\n",
    "def tversky(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "    y_true_pos = K.flatten(y_true)\n",
    "    y_pred_pos = K.flatten(y_pred)\n",
    "    true_pos = K.sum(y_true_pos * y_pred_pos)\n",
    "    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n",
    "    false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n",
    "    alpha = 0.7\n",
    "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
    "\n",
    "def tversky_loss(y_true, y_pred):\n",
    "    return 1 - tversky(y_true,y_pred)\n",
    "\n",
    "def focal_tversky(y_true,y_pred):\n",
    "    pt_1 = tversky(y_true, y_pred)\n",
    "    gamma = 0.75\n",
    "    return K.pow((1-pt_1), gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "\n",
      "Block 1 -> input: (None, 224, 224) output: (None, 112, 112, 32)\n",
      "Block 2 -> input: (None, 112, 112, 32) output: (None, 56, 56, 64)\n",
      "Block 3 -> input: (None, 56, 56, 64) output: (None, 28, 28, 128)\n",
      "Block 4 -> input: (None, 28, 28, 128) output: (None, 14, 14, 256)\n",
      "Block 7 -> input: (None, 14, 14, 256) output: (None, 28, 28, 128)\n",
      "Block 8 -> input: (None, 28, 28, 128) output: (None, 56, 56, 64)\n",
      "Block 9 -> input: (None, 56, 56, 64) output: (None, 112, 112, 32)\n",
      "\n",
      "\n",
      "DS 1 -> input: (None, 56, 56, 128) output: (None, 56, 56, 4)\n",
      "DS 2 -> input: (None, 112, 112, 64) output: (None, 112, 112, 4)\n",
      "DS 3 -> input: (None, 224, 224, 32) output: (None, 224, 224, 4)\n",
      "Epoch 1/120\n",
      "134/134 [==============================] - 84s 500ms/step - loss: 0.3282 - pred1_loss: 0.1721 - pred2_loss: 0.2156 - final_loss: 0.4237\n",
      "Epoch 2/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0834 - pred1_loss: 0.0794 - pred2_loss: 0.0798 - final_loss: 0.0863\n",
      "Epoch 3/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0556 - pred1_loss: 0.0594 - pred2_loss: 0.0551 - final_loss: 0.0548\n",
      "Epoch 4/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0396 - pred1_loss: 0.0452 - pred2_loss: 0.0398 - final_loss: 0.0381\n",
      "Epoch 5/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0320 - pred1_loss: 0.0387 - pred2_loss: 0.0322 - final_loss: 0.0302\n",
      "Epoch 6/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0316 - pred1_loss: 0.0398 - pred2_loss: 0.0321 - final_loss: 0.0294\n",
      "Epoch 7/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0274 - pred1_loss: 0.0358 - pred2_loss: 0.0274 - final_loss: 0.0254\n",
      "Epoch 8/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0266 - pred1_loss: 0.0360 - pred2_loss: 0.0266 - final_loss: 0.0243\n",
      "Epoch 9/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0238 - pred1_loss: 0.0345 - pred2_loss: 0.0239 - final_loss: 0.0212\n",
      "Epoch 10/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0220 - pred1_loss: 0.0334 - pred2_loss: 0.0217 - final_loss: 0.0194\n",
      "Epoch 11/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0184 - pred1_loss: 0.0294 - pred2_loss: 0.0184 - final_loss: 0.0158\n",
      "Epoch 12/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0186 - pred1_loss: 0.0298 - pred2_loss: 0.0185 - final_loss: 0.0158\n",
      "Epoch 13/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0170 - pred1_loss: 0.0278 - pred2_loss: 0.0170 - final_loss: 0.0144\n",
      "Epoch 14/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0159 - pred1_loss: 0.0270 - pred2_loss: 0.0158 - final_loss: 0.0132\n",
      "Epoch 15/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0153 - pred1_loss: 0.0268 - pred2_loss: 0.0153 - final_loss: 0.0125\n",
      "Epoch 16/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0143 - pred1_loss: 0.0257 - pred2_loss: 0.0143 - final_loss: 0.0115\n",
      "Epoch 17/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0144 - pred1_loss: 0.0263 - pred2_loss: 0.0144 - final_loss: 0.0115\n",
      "Epoch 18/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0140 - pred1_loss: 0.0253 - pred2_loss: 0.0140 - final_loss: 0.0112\n",
      "Epoch 19/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0136 - pred1_loss: 0.0251 - pred2_loss: 0.0137 - final_loss: 0.0108\n",
      "Epoch 20/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0146 - pred1_loss: 0.0265 - pred2_loss: 0.0146 - final_loss: 0.0116\n",
      "Epoch 21/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0128 - pred1_loss: 0.0245 - pred2_loss: 0.0129 - final_loss: 0.0098\n",
      "Epoch 22/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0142 - pred1_loss: 0.0257 - pred2_loss: 0.0142 - final_loss: 0.0113\n",
      "Epoch 23/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0128 - pred1_loss: 0.0245 - pred2_loss: 0.0129 - final_loss: 0.0099\n",
      "Epoch 24/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0143 - pred1_loss: 0.0266 - pred2_loss: 0.0142 - final_loss: 0.0113\n",
      "Epoch 25/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0134 - pred1_loss: 0.0251 - pred2_loss: 0.0135 - final_loss: 0.0105\n",
      "Epoch 26/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0141 - pred1_loss: 0.0262 - pred2_loss: 0.0142 - final_loss: 0.0111\n",
      "Epoch 27/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0131 - pred1_loss: 0.0253 - pred2_loss: 0.0132 - final_loss: 0.0100\n",
      "Epoch 28/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0135 - pred1_loss: 0.0254 - pred2_loss: 0.0135 - final_loss: 0.0105\n",
      "Epoch 29/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0136 - pred1_loss: 0.0262 - pred2_loss: 0.0136 - final_loss: 0.0105\n",
      "Epoch 30/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0124 - pred1_loss: 0.0248 - pred2_loss: 0.0125 - final_loss: 0.0094\n",
      "Epoch 31/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0129 - pred1_loss: 0.0257 - pred2_loss: 0.0130 - final_loss: 0.0098\n",
      "Epoch 32/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0124 - pred1_loss: 0.0246 - pred2_loss: 0.0125 - final_loss: 0.0093\n",
      "Epoch 33/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0117 - pred1_loss: 0.0236 - pred2_loss: 0.0119 - final_loss: 0.0087\n",
      "Epoch 34/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0119 - pred1_loss: 0.0235 - pred2_loss: 0.0121 - final_loss: 0.0089\n",
      "Epoch 35/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0111 - pred1_loss: 0.0226 - pred2_loss: 0.0113 - final_loss: 0.0081\n",
      "Epoch 36/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0111 - pred1_loss: 0.0226 - pred2_loss: 0.0113 - final_loss: 0.0081\n",
      "Epoch 37/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0114 - pred1_loss: 0.0230 - pred2_loss: 0.0116 - final_loss: 0.0085\n",
      "Epoch 38/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0122 - pred1_loss: 0.0242 - pred2_loss: 0.0123 - final_loss: 0.0092\n",
      "Epoch 39/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0125 - pred1_loss: 0.0246 - pred2_loss: 0.0127 - final_loss: 0.0094\n",
      "Epoch 40/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0115 - pred1_loss: 0.0227 - pred2_loss: 0.0117 - final_loss: 0.0086\n",
      "Epoch 41/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0112 - pred1_loss: 0.0227 - pred2_loss: 0.0114 - final_loss: 0.0082\n",
      "Epoch 42/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0105 - pred1_loss: 0.0218 - pred2_loss: 0.0108 - final_loss: 0.0076\n",
      "Epoch 43/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0106 - pred1_loss: 0.0218 - pred2_loss: 0.0109 - final_loss: 0.0076\n",
      "Epoch 44/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0111 - pred1_loss: 0.0232 - pred2_loss: 0.0113 - final_loss: 0.0080\n",
      "Epoch 45/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0167 - pred1_loss: 0.0303 - pred2_loss: 0.0167 - final_loss: 0.0134\n",
      "Epoch 46/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0120 - pred1_loss: 0.0243 - pred2_loss: 0.0121 - final_loss: 0.0089\n",
      "Epoch 47/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0114 - pred1_loss: 0.0231 - pred2_loss: 0.0117 - final_loss: 0.0083\n",
      "Epoch 48/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0110 - pred1_loss: 0.0226 - pred2_loss: 0.0113 - final_loss: 0.0081\n",
      "Epoch 49/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0108 - pred1_loss: 0.0223 - pred2_loss: 0.0111 - final_loss: 0.0079\n",
      "Epoch 50/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0104 - pred1_loss: 0.0218 - pred2_loss: 0.0106 - final_loss: 0.0074\n",
      "Epoch 51/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0116 - pred1_loss: 0.0235 - pred2_loss: 0.0118 - final_loss: 0.0086\n",
      "Epoch 52/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0110 - pred1_loss: 0.0226 - pred2_loss: 0.0112 - final_loss: 0.0081\n",
      "Epoch 53/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0101 - pred1_loss: 0.0214 - pred2_loss: 0.0104 - final_loss: 0.0072\n",
      "Epoch 54/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0099 - pred1_loss: 0.0214 - pred2_loss: 0.0103 - final_loss: 0.0070\n",
      "Epoch 55/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0102 - pred1_loss: 0.0215 - pred2_loss: 0.0105 - final_loss: 0.0072\n",
      "Epoch 56/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0100 - pred1_loss: 0.0213 - pred2_loss: 0.0103 - final_loss: 0.0070\n",
      "Epoch 57/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0105 - pred1_loss: 0.0220 - pred2_loss: 0.0107 - final_loss: 0.0075\n",
      "Epoch 58/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0104 - pred1_loss: 0.0216 - pred2_loss: 0.0107 - final_loss: 0.0074\n",
      "Epoch 59/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0111 - pred1_loss: 0.0231 - pred2_loss: 0.0114 - final_loss: 0.0080\n",
      "Epoch 60/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0119 - pred1_loss: 0.0242 - pred2_loss: 0.0121 - final_loss: 0.0088\n",
      "Epoch 61/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0104 - pred1_loss: 0.0222 - pred2_loss: 0.0107 - final_loss: 0.0073\n",
      "Epoch 62/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0103 - pred1_loss: 0.0218 - pred2_loss: 0.0107 - final_loss: 0.0072\n",
      "Epoch 63/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0104 - pred1_loss: 0.0217 - pred2_loss: 0.0107 - final_loss: 0.0074\n",
      "Epoch 64/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0114 - pred1_loss: 0.0231 - pred2_loss: 0.0117 - final_loss: 0.0083\n",
      "Epoch 65/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0104 - pred1_loss: 0.0218 - pred2_loss: 0.0108 - final_loss: 0.0075\n",
      "Epoch 66/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0105 - pred1_loss: 0.0219 - pred2_loss: 0.0109 - final_loss: 0.0075\n",
      "Epoch 67/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0103 - pred1_loss: 0.0219 - pred2_loss: 0.0107 - final_loss: 0.0073\n",
      "Epoch 68/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0099 - pred1_loss: 0.0211 - pred2_loss: 0.0103 - final_loss: 0.0070\n",
      "Epoch 69/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0101 - pred1_loss: 0.0217 - pred2_loss: 0.0104 - final_loss: 0.0071\n",
      "Epoch 70/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0099 - pred1_loss: 0.0212 - pred2_loss: 0.0102 - final_loss: 0.0069\n",
      "Epoch 71/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0117 - pred1_loss: 0.0235 - pred2_loss: 0.0119 - final_loss: 0.0087\n",
      "Epoch 72/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0101 - pred1_loss: 0.0219 - pred2_loss: 0.0105 - final_loss: 0.0070\n",
      "Epoch 73/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0106 - pred1_loss: 0.0222 - pred2_loss: 0.0110 - final_loss: 0.0075\n",
      "Epoch 74/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0098 - pred1_loss: 0.0212 - pred2_loss: 0.0101 - final_loss: 0.0068\n",
      "Epoch 75/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0097 - pred1_loss: 0.0209 - pred2_loss: 0.0101 - final_loss: 0.0068\n",
      "Epoch 76/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0098 - pred1_loss: 0.0211 - pred2_loss: 0.0102 - final_loss: 0.0068\n",
      "Epoch 77/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0097 - pred1_loss: 0.0214 - pred2_loss: 0.0100 - final_loss: 0.0067\n",
      "Epoch 78/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0095 - pred1_loss: 0.0209 - pred2_loss: 0.0099 - final_loss: 0.0065\n",
      "Epoch 79/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0104 - pred1_loss: 0.0216 - pred2_loss: 0.0107 - final_loss: 0.0074\n",
      "Epoch 80/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0102 - pred1_loss: 0.0213 - pred2_loss: 0.0106 - final_loss: 0.0073\n",
      "Epoch 81/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0094 - pred1_loss: 0.0204 - pred2_loss: 0.0098 - final_loss: 0.0065\n",
      "Epoch 82/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0106 - pred1_loss: 0.0225 - pred2_loss: 0.0109 - final_loss: 0.0076\n",
      "Epoch 83/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0106 - pred1_loss: 0.0226 - pred2_loss: 0.0109 - final_loss: 0.0076\n",
      "Epoch 84/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0096 - pred1_loss: 0.0208 - pred2_loss: 0.0100 - final_loss: 0.0066\n",
      "Epoch 85/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0104 - pred1_loss: 0.0217 - pred2_loss: 0.0108 - final_loss: 0.0074\n",
      "Epoch 86/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0095 - pred1_loss: 0.0207 - pred2_loss: 0.0099 - final_loss: 0.0066\n",
      "Epoch 87/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0096 - pred1_loss: 0.0207 - pred2_loss: 0.0101 - final_loss: 0.0067\n",
      "Epoch 88/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0094 - pred1_loss: 0.0207 - pred2_loss: 0.0099 - final_loss: 0.0064\n",
      "Epoch 89/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0094 - pred1_loss: 0.0206 - pred2_loss: 0.0098 - final_loss: 0.0065\n",
      "Epoch 90/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0093 - pred1_loss: 0.0206 - pred2_loss: 0.0097 - final_loss: 0.0064\n",
      "Epoch 91/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0094 - pred1_loss: 0.0203 - pred2_loss: 0.0098 - final_loss: 0.0065\n",
      "Epoch 92/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0095 - pred1_loss: 0.0205 - pred2_loss: 0.0098 - final_loss: 0.0065\n",
      "Epoch 93/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0095 - pred1_loss: 0.0206 - pred2_loss: 0.0099 - final_loss: 0.0065\n",
      "Epoch 94/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0094 - pred1_loss: 0.0203 - pred2_loss: 0.0098 - final_loss: 0.0066\n",
      "Epoch 95/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0090 - pred1_loss: 0.0200 - pred2_loss: 0.0096 - final_loss: 0.0061\n",
      "Epoch 96/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0098 - pred1_loss: 0.0207 - pred2_loss: 0.0101 - final_loss: 0.0069\n",
      "Epoch 97/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0093 - pred1_loss: 0.0204 - pred2_loss: 0.0098 - final_loss: 0.0063\n",
      "Epoch 98/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0092 - pred1_loss: 0.0203 - pred2_loss: 0.0097 - final_loss: 0.0063\n",
      "Epoch 99/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0090 - pred1_loss: 0.0201 - pred2_loss: 0.0095 - final_loss: 0.0061\n",
      "Epoch 100/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0094 - pred1_loss: 0.0204 - pred2_loss: 0.0098 - final_loss: 0.0064\n",
      "Epoch 101/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0102 - pred1_loss: 0.0218 - pred2_loss: 0.0107 - final_loss: 0.0072\n",
      "Epoch 102/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0095 - pred1_loss: 0.0206 - pred2_loss: 0.0099 - final_loss: 0.0066\n",
      "Epoch 103/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0103 - pred1_loss: 0.0216 - pred2_loss: 0.0107 - final_loss: 0.0073\n",
      "Epoch 104/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0091 - pred1_loss: 0.0196 - pred2_loss: 0.0095 - final_loss: 0.0063\n",
      "Epoch 105/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0090 - pred1_loss: 0.0196 - pred2_loss: 0.0094 - final_loss: 0.0061\n",
      "Epoch 106/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0088 - pred1_loss: 0.0197 - pred2_loss: 0.0092 - final_loss: 0.0060\n",
      "Epoch 107/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0095 - pred1_loss: 0.0206 - pred2_loss: 0.0100 - final_loss: 0.0066\n",
      "Epoch 108/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0094 - pred1_loss: 0.0205 - pred2_loss: 0.0099 - final_loss: 0.0064\n",
      "Epoch 109/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0092 - pred1_loss: 0.0204 - pred2_loss: 0.0096 - final_loss: 0.0062\n",
      "Epoch 110/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0089 - pred1_loss: 0.0198 - pred2_loss: 0.0094 - final_loss: 0.0060\n",
      "Epoch 111/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0094 - pred1_loss: 0.0206 - pred2_loss: 0.0099 - final_loss: 0.0064\n",
      "Epoch 112/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0093 - pred1_loss: 0.0205 - pred2_loss: 0.0096 - final_loss: 0.0063\n",
      "Epoch 113/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0089 - pred1_loss: 0.0198 - pred2_loss: 0.0093 - final_loss: 0.0059\n",
      "Epoch 114/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0091 - pred1_loss: 0.0202 - pred2_loss: 0.0095 - final_loss: 0.0062\n",
      "Epoch 115/120\n",
      "134/134 [==============================] - 67s 500ms/step - loss: 0.0092 - pred1_loss: 0.0205 - pred2_loss: 0.0096 - final_loss: 0.0062\n",
      "Epoch 116/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0096 - pred1_loss: 0.0207 - pred2_loss: 0.0101 - final_loss: 0.0066\n",
      "Epoch 117/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0095 - pred1_loss: 0.0205 - pred2_loss: 0.0100 - final_loss: 0.0066\n",
      "Epoch 118/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0093 - pred1_loss: 0.0205 - pred2_loss: 0.0097 - final_loss: 0.0063\n",
      "Epoch 119/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0088 - pred1_loss: 0.0197 - pred2_loss: 0.0093 - final_loss: 0.0060\n",
      "Epoch 120/120\n",
      "134/134 [==============================] - 67s 499ms/step - loss: 0.0090 - pred1_loss: 0.0198 - pred2_loss: 0.0094 - final_loss: 0.0061\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/30\n",
      "134/134 [==============================] - 75s 563ms/step - loss: 0.0091 - pred1_loss: 0.0199 - pred2_loss: 0.0095 - final_loss: 0.0062 - val_loss: 0.0104 - val_pred1_loss: 0.0204 - val_pred2_loss: 0.0112 - val_final_loss: 0.0074\n",
      "Epoch 2/30\n",
      "134/134 [==============================] - 70s 519ms/step - loss: 0.0092 - pred1_loss: 0.0201 - pred2_loss: 0.0096 - final_loss: 0.0063 - val_loss: 0.0104 - val_pred1_loss: 0.0211 - val_pred2_loss: 0.0115 - val_final_loss: 0.0072\n",
      "Epoch 3/30\n",
      "134/134 [==============================] - 70s 519ms/step - loss: 0.0087 - pred1_loss: 0.0195 - pred2_loss: 0.0092 - final_loss: 0.0059 - val_loss: 0.0108 - val_pred1_loss: 0.0231 - val_pred2_loss: 0.0119 - val_final_loss: 0.0071\n",
      "Epoch 4/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0084 - pred1_loss: 0.0193 - pred2_loss: 0.0089 - final_loss: 0.0055 - val_loss: 0.0108 - val_pred1_loss: 0.0214 - val_pred2_loss: 0.0119 - val_final_loss: 0.0077\n",
      "Epoch 5/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0091 - pred1_loss: 0.0203 - pred2_loss: 0.0096 - final_loss: 0.0062 - val_loss: 0.0125 - val_pred1_loss: 0.0235 - val_pred2_loss: 0.0137 - val_final_loss: 0.0092\n",
      "Epoch 6/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0091 - pred1_loss: 0.0199 - pred2_loss: 0.0096 - final_loss: 0.0062 - val_loss: 0.0109 - val_pred1_loss: 0.0222 - val_pred2_loss: 0.0120 - val_final_loss: 0.0076\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 7/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0085 - pred1_loss: 0.0194 - pred2_loss: 0.0089 - final_loss: 0.0056 - val_loss: 0.0103 - val_pred1_loss: 0.0205 - val_pred2_loss: 0.0115 - val_final_loss: 0.0072\n",
      "Epoch 8/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0083 - pred1_loss: 0.0191 - pred2_loss: 0.0088 - final_loss: 0.0054 - val_loss: 0.0109 - val_pred1_loss: 0.0215 - val_pred2_loss: 0.0120 - val_final_loss: 0.0078\n",
      "Epoch 9/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0083 - pred1_loss: 0.0191 - pred2_loss: 0.0088 - final_loss: 0.0054 - val_loss: 0.0104 - val_pred1_loss: 0.0210 - val_pred2_loss: 0.0115 - val_final_loss: 0.0072\n",
      "Epoch 10/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0081 - pred1_loss: 0.0190 - pred2_loss: 0.0087 - final_loss: 0.0052 - val_loss: 0.0095 - val_pred1_loss: 0.0205 - val_pred2_loss: 0.0104 - val_final_loss: 0.0064\n",
      "Epoch 11/30\n",
      "134/134 [==============================] - 70s 519ms/step - loss: 0.0082 - pred1_loss: 0.0188 - pred2_loss: 0.0087 - final_loss: 0.0053 - val_loss: 0.0106 - val_pred1_loss: 0.0206 - val_pred2_loss: 0.0117 - val_final_loss: 0.0075\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 12/30\n",
      "134/134 [==============================] - 70s 519ms/step - loss: 0.0081 - pred1_loss: 0.0188 - pred2_loss: 0.0086 - final_loss: 0.0052 - val_loss: 0.0105 - val_pred1_loss: 0.0212 - val_pred2_loss: 0.0117 - val_final_loss: 0.0073\n",
      "Epoch 13/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0080 - pred1_loss: 0.0186 - pred2_loss: 0.0085 - final_loss: 0.0051 - val_loss: 0.0104 - val_pred1_loss: 0.0216 - val_pred2_loss: 0.0115 - val_final_loss: 0.0070\n",
      "Epoch 14/30\n",
      "134/134 [==============================] - 70s 519ms/step - loss: 0.0081 - pred1_loss: 0.0188 - pred2_loss: 0.0086 - final_loss: 0.0052 - val_loss: 0.0097 - val_pred1_loss: 0.0196 - val_pred2_loss: 0.0109 - val_final_loss: 0.0066\n",
      "Epoch 15/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0080 - pred1_loss: 0.0187 - pred2_loss: 0.0085 - final_loss: 0.0051 - val_loss: 0.0100 - val_pred1_loss: 0.0209 - val_pred2_loss: 0.0111 - val_final_loss: 0.0068\n",
      "Epoch 16/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0078 - pred1_loss: 0.0183 - pred2_loss: 0.0083 - final_loss: 0.0049 - val_loss: 0.0092 - val_pred1_loss: 0.0189 - val_pred2_loss: 0.0102 - val_final_loss: 0.0064\n",
      "Epoch 17/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0079 - pred1_loss: 0.0185 - pred2_loss: 0.0084 - final_loss: 0.0050 - val_loss: 0.0102 - val_pred1_loss: 0.0212 - val_pred2_loss: 0.0114 - val_final_loss: 0.0069\n",
      "Epoch 18/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0080 - pred1_loss: 0.0186 - pred2_loss: 0.0084 - final_loss: 0.0051 - val_loss: 0.0099 - val_pred1_loss: 0.0208 - val_pred2_loss: 0.0109 - val_final_loss: 0.0066\n",
      "Epoch 19/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0078 - pred1_loss: 0.0185 - pred2_loss: 0.0083 - final_loss: 0.0050 - val_loss: 0.0101 - val_pred1_loss: 0.0206 - val_pred2_loss: 0.0112 - val_final_loss: 0.0070\n",
      "Epoch 20/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0079 - pred1_loss: 0.0186 - pred2_loss: 0.0085 - final_loss: 0.0050 - val_loss: 0.0101 - val_pred1_loss: 0.0208 - val_pred2_loss: 0.0110 - val_final_loss: 0.0070\n",
      "Epoch 21/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0078 - pred1_loss: 0.0182 - pred2_loss: 0.0083 - final_loss: 0.0050 - val_loss: 0.0108 - val_pred1_loss: 0.0217 - val_pred2_loss: 0.0121 - val_final_loss: 0.0075\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 22/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0077 - pred1_loss: 0.0183 - pred2_loss: 0.0082 - final_loss: 0.0049 - val_loss: 0.0101 - val_pred1_loss: 0.0211 - val_pred2_loss: 0.0112 - val_final_loss: 0.0068\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0078 - pred1_loss: 0.0182 - pred2_loss: 0.0083 - final_loss: 0.0049 - val_loss: 0.0104 - val_pred1_loss: 0.0217 - val_pred2_loss: 0.0114 - val_final_loss: 0.0071\n",
      "Epoch 24/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0079 - pred1_loss: 0.0186 - pred2_loss: 0.0084 - final_loss: 0.0050 - val_loss: 0.0100 - val_pred1_loss: 0.0207 - val_pred2_loss: 0.0110 - val_final_loss: 0.0069\n",
      "Epoch 25/30\n",
      "134/134 [==============================] - 70s 519ms/step - loss: 0.0076 - pred1_loss: 0.0182 - pred2_loss: 0.0082 - final_loss: 0.0048 - val_loss: 0.0099 - val_pred1_loss: 0.0200 - val_pred2_loss: 0.0110 - val_final_loss: 0.0070\n",
      "Epoch 26/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0078 - pred1_loss: 0.0184 - pred2_loss: 0.0083 - final_loss: 0.0049 - val_loss: 0.0100 - val_pred1_loss: 0.0204 - val_pred2_loss: 0.0110 - val_final_loss: 0.0069\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 27/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0077 - pred1_loss: 0.0184 - pred2_loss: 0.0083 - final_loss: 0.0049 - val_loss: 0.0105 - val_pred1_loss: 0.0211 - val_pred2_loss: 0.0113 - val_final_loss: 0.0074\n",
      "Epoch 28/30\n",
      "134/134 [==============================] - 70s 519ms/step - loss: 0.0077 - pred1_loss: 0.0182 - pred2_loss: 0.0082 - final_loss: 0.0048 - val_loss: 0.0098 - val_pred1_loss: 0.0205 - val_pred2_loss: 0.0109 - val_final_loss: 0.0066\n",
      "Epoch 29/30\n",
      "134/134 [==============================] - 70s 519ms/step - loss: 0.0077 - pred1_loss: 0.0183 - pred2_loss: 0.0082 - final_loss: 0.0049 - val_loss: 0.0100 - val_pred1_loss: 0.0203 - val_pred2_loss: 0.0109 - val_final_loss: 0.0069\n",
      "Epoch 30/30\n",
      "134/134 [==============================] - 70s 520ms/step - loss: 0.0076 - pred1_loss: 0.0181 - pred2_loss: 0.0081 - final_loss: 0.0048 - val_loss: 0.0100 - val_pred1_loss: 0.0210 - val_pred2_loss: 0.0110 - val_final_loss: 0.0067\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "\n",
    "    model = myunet(X_train)\n",
    "\n",
    "    name = \"myUNet\".format(time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\"myunet_tflogs/{}\".format(name))\n",
    "\n",
    "    warmup_epoch = 8\n",
    "    warmup_run_epochs = 120\n",
    "    normal_run_epochs = 30\n",
    "\n",
    "    rlrop = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        mode='min', \n",
    "        patience=5,\n",
    "        factor=.5, \n",
    "        # min_lr=1e-6, \n",
    "        min_delta=.001,\n",
    "        verbose=1)\n",
    "\n",
    "    checkpoint_filepath = os.getcwd()+\"/acdc_myunet_weights.h5\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True\n",
    "        )\n",
    "\n",
    "    earlystop = keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                restore_best_weights=True,\n",
    "                min_delta=.001,\n",
    "                patience=12)\n",
    "\n",
    "    lr = 1e-3\n",
    "    opt = tf.keras.optimizers.Adam(lr = lr)\n",
    "\n",
    "    # Compute the number of warmup batches\n",
    "    warmup_batches = warmup_epoch * len(X_train)//batch_size\n",
    "    # Create the Learning rate scheduler\n",
    "    warm_up_lr = WarmUpLearningRateScheduler(warmup_batches, init_lr=lr)\n",
    "    \n",
    "    loss = {'pred1':bce_dice_loss,\n",
    "            'pred2':bce_dice_loss,\n",
    "            'final':bce_dice_loss}\n",
    "\n",
    "    loss_weights = {'pred1':.14,\n",
    "                    'pred2':.29,\n",
    "                    'final':.57}   \n",
    "\n",
    "\n",
    "\n",
    "    loss1 = \"binary_crossentropy\"\n",
    "\n",
    "    model.compile(optimizer = opt, \n",
    "                  loss = [loss1, loss1, loss1],\n",
    "                  loss_weights=[.14, .29, .57],\n",
    "                  #metrics = [dsc]\n",
    "                 )  \n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch = len(X_train)//batch_size, \n",
    "    epochs=warmup_run_epochs,\n",
    "    callbacks=[warm_up_lr],\n",
    ")\n",
    "\n",
    "\n",
    "np.save('acdc_history_warmup_bestmodel.npy',history.history)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data = val_generator,\n",
    "    steps_per_epoch = len(X_train)//batch_size,\n",
    "    validation_steps = len(X_val)//batch_size,\n",
    "    epochs=normal_run_epochs,\n",
    "    callbacks = [rlrop, checkpoint_callback, tensorboard_callback],\n",
    "    \n",
    "          )\n",
    "\n",
    "np.save('acdc_history_rlrop_bestmodel.npy',history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc3d8220ac8>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history=np.load(\"acdc_history_warmup_bestmodel.npy\",allow_pickle='TRUE').item()\n",
    "plt.plot(history[\"loss\"])\n",
    "plt.show()\n",
    "history=np.load(\"acdc_history_rlrop_bestmodel.npy\",allow_pickle='TRUE').item()\n",
    "plt.plot(history[\"loss\"])\n",
    "plt.plot(history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9589 0.9202 0.906 ]\n",
      "0.9284\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(X_test, batch_size=1)\n",
    "\n",
    "print(np.round(np.array(metrics(y_test[:,:,:,-1], np.argmax(predicted[2], axis=3),0)),4))\n",
    "print(np.round(np.array(metrics(y_test[:,:,:,-1], np.argmax(predicted[2], axis=3),0)).mean(),4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
